# Evaluation Script Updates - L1/L2 Support

## λ³€κ²½ μ‚¬ν•­ μ”μ•½

`evaluate_classifier.py` μ¤ν¬λ¦½νΈλ¥Ό μ—…λ°μ΄νΈν•μ—¬ L1κ³Ό L2 λ¶„λ¥λ¥Ό λ¨λ‘ ν‰κ°€ν•  μ μλ„λ΅ κ°μ„ ν–μµλ‹λ‹¤.

## μ£Όμ” λ³€κ²½μ‚¬ν•­

### 1. `evaluate_classifier()` ν•¨μ μ—…λ°μ΄νΈ

**μ¶”κ°€λ νλΌλ―Έν„°:**
```python
classification_level: str = "L1"
```

**λ³€κ²½λ λ™μ‘:**
- λ¶„λ¥ μμ¤€μ„ μ¶λ ¥μ— ν‘μ‹
- `DocumentClassifier` μ΄κΈ°ν™” μ‹ `classification_level` μ „λ‹¬
- λ™μ μΌλ΅ μ¬λ°”λ¥Έ μΉ΄ν…κ³ λ¦¬ μ»¬λΌ μ„ νƒ:
  ```python
  category_column = "categoryL1" if classification_level == "L1" else "categoryL2"
  actual_category = row[category_column]
  ```

### 2. `main()` ν•¨μ - μ»¤λ§¨λ“λΌμΈ μΈμ μ§€μ›

**μƒλ΅μ΄ argparse μ„¤μ •:**

| μΈμ | νƒ€μ… | κΈ°λ³Έκ°’ | μ„¤λ… |
|------|------|--------|------|
| `--level` | `{L1, L2}` | `L1` | λ¶„λ¥ μμ¤€ μ„ νƒ |
| `--sample-size` | `int` | `None` | ν‰κ°€ν•  μƒν” μ |
| `--no-examples` | `flag` | `False` | Few-shot μμ‹ λΉ„ν™μ„±ν™” |
| `--seed` | `int` | `42` | λλ¤ μ‹λ“ |

**νμΌ μ €μ¥ κ²½λ΅ λ³€κ²½:**
- κΈ°μ΅΄: `evaluation_results.csv`
- λ³€κ²½:
  - L1: `evaluation_results_l1.csv`
  - L2: `evaluation_results_l2.csv`

### 3. λ¬Έμ„ν™” μ—…λ°μ΄νΈ

**νμΌ ν—¤λ” λ…μ¤νΈλ§:**
- L1/L2 μ§€μ› λ…μ‹
- μ‚¬μ© μμ‹ μ¶”κ°€

## μ½”λ“ λΉ„κµ

### Before (L1λ§ μ§€μ›)
```python
def evaluate_classifier(
    sample_size: int = None,
    use_examples: bool = True,
    random_seed: int = 42,
) -> Tuple[pd.DataFrame, Dict[str, float]]:
    # ...
    classifier = DocumentClassifier(model="gpt-4o-mini")
    # ...
    actual_category = row["categoryL1"]  # ν•λ“μ½”λ”©λ¨
```

### After (L1/L2 μ§€μ›)
```python
def evaluate_classifier(
    sample_size: int = None,
    use_examples: bool = True,
    random_seed: int = 42,
    classification_level: str = "L1",  # μƒλ΅μ΄ νλΌλ―Έν„°
) -> Tuple[pd.DataFrame, Dict[str, float]]:
    # ...
    classifier = DocumentClassifier(
        model="gpt-4o-mini",
        classification_level=classification_level  # λ™μ  μ„¤μ •
    )
    # ...
    category_column = "categoryL1" if classification_level == "L1" else "categoryL2"
    actual_category = row[category_column]  # λ™μ  μ„ νƒ
```

## μ‚¬μ© μμ‹

### μ»¤λ§¨λ“λΌμΈ μ‚¬μ©

```bash
# L1 ν‰κ°€ (μ „μ²΄ λ°μ΄ν„°)
python examples/evaluate_classifier.py --level L1

# L2 ν‰κ°€ (20κ° μƒν”)
python examples/evaluate_classifier.py --level L2 --sample-size 20

# Few-shot μ—†μ΄ ν‰κ°€
python examples/evaluate_classifier.py --level L1 --no-examples

# μ»¤μ¤ν…€ μ‹λ“λ΅ μƒν”λ§
python examples/evaluate_classifier.py --level L2 --sample-size 30 --seed 999
```

### ν”„λ΅κ·Έλλ° λ°©μ‹ μ‚¬μ©

```python
from evaluate_classifier import evaluate_classifier

# L1 ν‰κ°€
results_l1, metrics_l1 = evaluate_classifier(
    sample_size=20,
    classification_level="L1"
)

# L2 ν‰κ°€
results_l2, metrics_l2 = evaluate_classifier(
    sample_size=20,
    classification_level="L2"
)
```

## μ¶λ ¥ μμ‹

```bash
$ python examples/evaluate_classifier.py --level L2 --sample-size 10

================================================================================
Document Classifier Evaluation
================================================================================
Total documents in dataset: 89
Classification level: L2
Evaluating on 10 sampled documents

Initializing classifier...

Classifying documents...
[1/10] Actual: μ—…λ¬΄ μ§€μ› | Predicted: μ—…λ¬΄ μ§€μ› | β“
[2/10] Actual: μƒν™ μ§€μ› | Predicted: μƒν™ μ§€μ› | β“
[3/10] Actual: λ¦¬λ”μ‹­ | Predicted: λ¦¬λ”μ‹­ | β“
...

================================================================================
EVALUATION RESULTS
================================================================================

Overall Accuracy: 80.00%
Correct: 8/10

--------------------------------------------------------------------------------
Per-Category Performance:
--------------------------------------------------------------------------------
μ—…λ¬΄ μ§€μ›                      | Accuracy: 100.00% (2/2)
μƒν™ μ§€μ›                      | Accuracy: 66.67% (2/3)
λ¦¬λ”μ‹­                        | Accuracy: 100.00% (1/1)
...

Results saved to: .../data/intermediate/evaluation_results_l2.csv
```

## μ¶”κ°€ μƒμ„±λ νμΌ

### `examples/EVALUATION_GUIDE.md`
μƒμ„Έν• ν‰κ°€ μ¤ν¬λ¦½νΈ μ‚¬μ© κ°€μ΄λ“:
- μ‚¬μ© λ°©λ²• λ° μµμ… μ„¤λ…
- μ¶λ ¥ κ²°κ³Ό ν•΄μ„ λ°©λ²•
- μ„±λ¥ κ°μ„  ν
- λ¬Έμ  ν•΄κ²° κ°€μ΄λ“

## ν•μ„ νΈν™μ„±

κΈ°μ΅΄ μ½”λ“μ™€μ ν•μ„ νΈν™μ„± μ μ§€:
- `classification_level` νλΌλ―Έν„°μ κΈ°λ³Έκ°’μ΄ `"L1"`μ΄λ―€λ΅ κΈ°μ΅΄ μ½”λ“λ” μμ • μ—†μ΄ μ‘λ™
- κΈ°μ΅΄ μ‚¬μ© λ°©μ‹:
  ```python
  # μ—¬μ „ν μ‘λ™ (κΈ°λ³Έμ μΌλ΅ L1 μ‚¬μ©)
  results, metrics = evaluate_classifier(sample_size=10)
  ```

## ν…μ¤νΈ μ™„λ£ ν•­λ©

β… Help λ©”μ‹μ§€ μ •μƒ μ¶λ ¥
β… argparse νλΌλ―Έν„° μ •μ μ™„λ£
β… L1/L2 λ™μ  λ¶„λ¥ μμ¤€ μ„ νƒ
β… λ™μ  μΉ΄ν…κ³ λ¦¬ μ»¬λΌ μ„ νƒ
β… νμΌλ… λ λ²¨λ³„ κµ¬λ¶„ μ €μ¥
β… λ¬Έμ„ν™” μ—…λ°μ΄νΈ

## λ‹¤μ λ‹¨κ³„ (μ„ νƒμ‚¬ν•­)

1. **μ‹¤μ  ν‰κ°€ μ‹¤ν–‰**: OpenAI API ν‚¤λ¥Ό μ„¤μ •ν•κ³  μ‹¤μ  ν‰κ°€ μν–‰
2. **μ„±λ¥ λΉ„κµ**: L1κ³Ό L2 μ„±λ¥ λΉ„κµ λ¶„μ„
3. **ν”„λ΅¬ν”„νΈ νλ‹**: ν‰κ°€ κ²°κ³Όλ¥Ό λ°”νƒ•μΌλ΅ ν”„λ΅¬ν”„νΈ κ°μ„ 
4. **μ‹κ°ν™”**: ν‰κ°€ κ²°κ³Όλ¥Ό κ·Έλν”„λ΅ μ‹κ°ν™”ν•λ” μ¤ν¬λ¦½νΈ μ¶”κ°€

## μ™„λ£! π‰

`evaluate_classifier.py`λ” μ΄μ  L1κ³Ό L2 λ¶„λ¥λ¥Ό λ¨λ‘ μ μ—°ν•κ² ν‰κ°€ν•  μ μμµλ‹λ‹¤.
